Introduction

In large language models, a prompt is the input provided to the model to produce an output. 
The input is typically text, but it can also be other forms of data such as images or a combination of different data types. 
Anyone can create a prompt, as there is no need to be a data scientist or a machine learning engineer however,
creating an effective prompt may not be straightforward.

The output generated by the model depends on several factors, including the model being used,
the training data of the model, the model’s settings, the choice of words, tone, and writing style,
the structure and clarity of the prompt, and the context provided in the input. Due to these factors, 
prompt engineering is an iterative process in which prompts are refined and improved over time based on the 
results produced by the model.

Poorly written prompts can lead to ambiguous and unclear responses, incorrect or misleading outputs,
and a decreased usefulness of the model’s answers. Therefore, prompt engineering focuses on designing prompts in a way that
ensures the model produces accurate, clear, and meaningful responses.

When you chat with the Gemini chatbot,1 you basically write prompts, however this 
whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  
the API, because by prompting the model directly you will have access to the configuration 
such as temperature etc.

This whitepaper discusses prompt engineering in detail. We will look into the various 
prompting techniques to help you getting started and share tips and best practices to 
become a prompting expert. We will also discuss some of the challenges you can face  
while crafting prompts.